
User_ID													: int

Product_ID												: une chaine, un ID

Gender													: F ou M

Age														: [0-17] [18-25] [26-35] [36-45] [46-50] [51-55] 55+ 

Occupation												: 1 nombre de 0 a 20

City_Category											: A,B,C

Stay_In_Current_City_Years								: 0 a 3 , 4+

Marital_Status											: 0,1

Product_Category_1										: 1 à 18					

Product_Category_2										: 2 à 18 ou vide

Product_Category_3										: 3 à 18 ou vide

Purchase												: int

-----------------------------------------------------------------------------------------------------------------------------------------------------
|																 Prétraitements 																	|
-----------------------------------------------------------------------------------------------------------------------------------------------------

Product_Category_1										: 1 à 18					

Product_Category_2										: 2 à 18 ou vide

Product_Category_3										: 3 à 18 ou vide

On avait 537577 sorties pour 3 categories de produits



Après: remplacement des valeurs manquantes, encodage et concatenation des categories.

On a 235 categories sur une seule colonne cat_all



User_ID													: int

Product_ID												: une chaine, un ID

Gender													: F ou M

Age														: [0-17] [18-25] [26-35] [36-45] [46-50] [51-55] 55+ 

Occupation												: 1 nombre de 0 a 20

City_Category											: A,B,C

Stay_In_Current_City_Years								: 0 a 3 , 4+

Marital_Status											: 0,1

Purchase												: int

Product_Category 										: str (235 valeurs)


Considérer que toutes les combinaisons d'une catégorie comme doublons (si se trouvant dans le dataset) et donc les standardiser en une seule catégorie.*

Supprimer le "User_ID" puisque c'est une information impertinente

*Ce traitement n'a fusionné aucune catégorie, il se pourrait que ce cas ait été traité au préalable.

-----------------------------------------------------------------------------------------------------------------------------------------------------
|																	 Models 																		|
-----------------------------------------------------------------------------------------------------------------------------------------------------



-----------------------------------------------------------------------------------------------------------------------------------------------------
|																 				 																	|
-----------------------------------------------------------------------------------------------------------------------------------------------------

SVM					=> N'a pas donné un bon résultat
Arbre de décision	=> A overfité puisqu'il traitait tous les cas et donc ne faisait qu'apprendre par coeur. Solution : Délimiter la profondeur de l'arbre pour lui permettre de généraliser.
Forêt aléatoire		=> Requiert une grande puissance de calcule

-----------------------------------------------------------------------------------------------------------------------------------------------------
|																 Perspectives	 																	|
-----------------------------------------------------------------------------------------------------------------------------------------------------


1. récolter plus de données.		====> Par rapport
2. utiliser la validation croisée.  ====>	au manque de données.
















